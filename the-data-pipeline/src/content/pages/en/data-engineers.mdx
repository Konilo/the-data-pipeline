---
title: "The Source of the Data Flow"
subtitle: "Data Engineers"
summaryRole: "Collect raw data that is useful for insights generation (after downstream processing) from a diversity of sources; setup the technical infrastructure used by the Data team"
summaryTools: "Scripting programming languages such as Python used to build the data ingestion pipelines"
prevPage:
  title: "Discovering the Data Pipeline"
  url: "data-pipeline"
nextPage:
  title: "Analytics Engineers"
  url: "analytics-engineers"
---
import Summary from '../../../components/Summary.astro';
export const { summaryRole, summaryTools } = frontmatter;

<Summary role={summaryRole} tools={summaryTools} />

### Contents

Organizations wish to leverage any data relevant to their activities. Therefore, the flow of data Data Professionals deal with originates from various sources. Depending on the organization, the source systems can be:

- The software the organization owns, develops, and distributes to clients whose actions on the software may be tracked (e.g. YouTube for Google);
- Software used by the organization, internally, to conduct its operations such as a customer relationship management system (e.g., Pipedrive), where the interactions between a company and its customers are logged, or an emailing platform (e.g., Mailjet) which sends emails and measures their effectiveness
- Third-party sources systems providing information which is valuable to the organization such as weather forecasts for organizations whose operations are affected by the climate (e.g., agri-food), or financial data for organizations which are affected by financial markets because they buy and/or sell commodities, for instance.

The collection of data from source systems is the realm of **Data Engineers**.

They develop the software necessary to collect this data in a timely and reliable manner â€“ this is the first "conduit" of the Data Pipeline.

Each source system differs from the others in a dimension or another. Those dimensions are:

- The authentication necessary to get the source system's trust and gain access to the data,
- The format of the data,
- The speed and frequency with which the data can be collected,
- The way the data can be read e.g., is it possible to read only some records of interest or is it only possible to download a dataset, which may be huge, completely?
- The fact that the data's structure can change over time whereas downstream operators of the Data Pipeline need consistency,
- Etc.

Data Engineers need to navigate this.

They then load the raw, unprocessed data as tables into a database centralizing all the data the organization wants to extract value from. Several designs of central databases exist with each their specificities but a common variant is the Data Warehouse. This is the main "storage facility" of the Data Pipeline. Data engineers are also responsible for the setup (choice of the database technology and deployment of the necessary infrastructure) and performance (reading and writing speeds) of this database.
